{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ludwig test\n",
    "## Installing virtualenv and ludwig on Ubuntu 18.04\n",
    "\n",
    "``` bash\n",
    "# Start from the text_classification directory\n",
    "cd text_classification\n",
    "\n",
    "# Get pip and virtualenv\n",
    "sudo apt-get install python3-pip\n",
    "pip3 install virtualenv\n",
    "\n",
    "# Now set up a new virtualenv with a new python\n",
    "virtualenv -p python3 venv\n",
    "# Activate the venv\n",
    ". venv/bin/activate\n",
    "# Note, to deactivate the venv, just run \"deactivate\"\n",
    "\n",
    "#Library prereq for ludwig 2\n",
    "sudo apt-get install libgmp3-dev\n",
    "sudo apt-get install libsndfile1\n",
    "\n",
    "# Now install packages and ludwig 0.2.2.8 (0.3 is buggy)\n",
    "pip install -r requirements.txt\n",
    "pip install ludwig==0.2.2.8\n",
    "pip install ludwig[viz]==0.2.2.8\n",
    "\n",
    "# Other missing dependencies\n",
    "pip install imblearn matplotlib\n",
    "\n",
    "# Get Jupyter (to run this file)\n",
    "pip install jupyter\n",
    "\n",
    "```\n",
    "\n",
    "## Splitting Moby Dick and Romeo and Juliet\n",
    "\n",
    "``` bash\n",
    "# Word wrap at 80 characters\n",
    "fold -w 80 -s moby_dick.txt\n",
    "\n",
    "# Now split the text every 5 lines, and prefix with a label\n",
    "cd book_data\n",
    "split -d -l 5 ../romeo_juliet.txt RJ_\n",
    "split -d -l 5 ../moby_dick.txt MD_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our Labeled Text into a CSV (via DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Cook,” said Stubb, rapidly lifting a rather r...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nA short space elapsed, and up into this nois...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said humorous Stubb one day, “he can never be ...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and portion off their nieces with a few porpoi...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as ordinary fish possess what is called a swim...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JULIET\\nNow, good sweet nurse,--O Lord, why lo...</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  “Cook,” said Stubb, rapidly lifting a rather r...    MD\n",
       "1  \\nA short space elapsed, and up into this nois...    MD\n",
       "2  said humorous Stubb one day, “he can never be ...    MD\n",
       "3  and portion off their nieces with a few porpoi...    MD\n",
       "4  as ordinary fish possess what is called a swim...    MD\n",
       "5  JULIET\\nNow, good sweet nurse,--O Lord, why lo...    RJ"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import the full collection of labeled texts\n",
    "#\n",
    "# All of the text is in \"book_data\" and begins with a label \"MD_\" for \"Moby Dick\" \n",
    "# or \"RJ_\" for Romeo and Juliet\n",
    "#\n",
    "\n",
    "filenames = []\n",
    "texts = []\n",
    "labels = []\n",
    "path = \"book_data\"\n",
    "# First let's\n",
    "for file in os.listdir(path):\n",
    "    if not file.startswith('.'):\n",
    "        filenames.append(file)\n",
    "        labels.append(file.split('_')[0])\n",
    "        with open(os.path.join(path,file), 'r') as f:\n",
    "            texts.append(f.read())\n",
    "\n",
    "# We read the text and insert it into a pandas dataframe. Sklearn uses dataframes\n",
    "# because it is a table class with nice analysis methods\n",
    "labeled_data = pd.DataFrame()\n",
    "labeled_data['text'] = texts\n",
    "labeled_data['label'] = labels\n",
    "\n",
    "# Display the first 6 rows to show that we successfully read it\n",
    "labeled_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,label\r\n",
      "\"“Cook,” said Stubb, rapidly lifting a rather reddish morsel to his mouth, \r\n",
      "“don’t you think this steak is rather overdone? You’ve been beating this \r\n",
      "steak too much, cook; it’s too tender. Don’t I always say that to be good, \r\n",
      "a whale-steak must be tough? There are those sharks now over the side, don’t \r\n",
      "you see they prefer it tough and rare? What a shindy they are kicking up! Cook, \r\n",
      "\",MD\r\n",
      "\"\r\n",
      "A short space elapsed, and up into this noiselessness came Ahab alone from his \r\n",
      "cabin. Taking a few turns on the quarter-deck, he paused to gaze over the side, \r\n"
     ]
    }
   ],
   "source": [
    "# Now save the data as a csv file\n",
    "labeled_data.to_csv(\"training_set.csv\", index=False)\n",
    "! head training_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ludwig to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 14 19:42:42 EDT 2020\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "███████████████████████\n",
      "█ █ █ █  ▜█ █ █ █ █   █\n",
      "█ █ █ █ █ █ █ █ █ █ ███\n",
      "█ █   █ █ █ █ █ █ █ ▌ █\n",
      "█ █████ █ █ █ █ █ █ █ █\n",
      "█     █  ▟█     █ █   █\n",
      "███████████████████████\n",
      "ludwig v0.2.2.8 - Experiment\n",
      "\n",
      "Experiment name: experiment\n",
      "Model name: run\n",
      "Output path: ./results/experiment_run\n",
      "\n",
      "\n",
      "ludwig_version: '0.2.2.8'\n",
      "command: ('/home/butch/machine_learning/text_classification/venv/bin/ludwig experiment '\n",
      " '--data_csv training_set.csv --model_definition_file model_definition.yaml '\n",
      " '--output_directory ./results')\n",
      "commit_hash: 'b23bfdf4968b'\n",
      "random_seed: 42\n",
      "input_data: 'training_set.csv'\n",
      "model_definition: {   'combiner': {'type': 'concat'},\n",
      "    'input_features': [   {   'encoder': 'parallel_cnn',\n",
      "                              'level': 'word',\n",
      "                              'name': 'text',\n",
      "                              'tied_weights': None,\n",
      "                              'type': 'text'}],\n",
      "    'output_features': [   {   'dependencies': [],\n",
      "                               'loss': {   'class_similarities_temperature': 0,\n",
      "                                           'class_weights': 1,\n",
      "                                           'confidence_penalty': 0,\n",
      "                                           'distortion': 1,\n",
      "                                           'labels_smoothing': 0,\n",
      "                                           'negative_samples': 0,\n",
      "                                           'robust_lambda': 0,\n",
      "                                           'sampler': None,\n",
      "                                           'type': 'softmax_cross_entropy',\n",
      "                                           'unique': False,\n",
      "                                           'weight': 1},\n",
      "                               'name': 'label',\n",
      "                               'reduce_dependencies': 'sum',\n",
      "                               'reduce_input': 'sum',\n",
      "                               'top_k': 3,\n",
      "                               'type': 'category'}],\n",
      "    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\n",
      "                                      'audio_file_length_limit_in_s': 7.5,\n",
      "                                      'in_memory': True,\n",
      "                                      'missing_value_strategy': 'backfill',\n",
      "                                      'norm': None,\n",
      "                                      'padding_value': 0},\n",
      "                         'bag': {   'fill_value': '<UNK>',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000,\n",
      "                                    'tokenizer': 'space'},\n",
      "                         'binary': {   'fill_value': 0,\n",
      "                                       'missing_value_strategy': 'fill_with_const'},\n",
      "                         'category': {   'fill_value': '<UNK>',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 10000},\n",
      "                         'date': {   'datetime_format': None,\n",
      "                                     'fill_value': '',\n",
      "                                     'missing_value_strategy': 'fill_with_const'},\n",
      "                         'force_split': False,\n",
      "                         'h3': {   'fill_value': 576495936675512319,\n",
      "                                   'missing_value_strategy': 'fill_with_const'},\n",
      "                         'image': {   'in_memory': True,\n",
      "                                      'missing_value_strategy': 'backfill',\n",
      "                                      'num_processes': 1,\n",
      "                                      'resize_method': 'interpolate',\n",
      "                                      'scaling': 'pixel_normalization'},\n",
      "                         'numerical': {   'fill_value': 0,\n",
      "                                          'missing_value_strategy': 'fill_with_const',\n",
      "                                          'normalization': None},\n",
      "                         'sequence': {   'fill_value': '<UNK>',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 20000,\n",
      "                                         'padding': 'right',\n",
      "                                         'padding_symbol': '<PAD>',\n",
      "                                         'sequence_length_limit': 256,\n",
      "                                         'tokenizer': 'space',\n",
      "                                         'unknown_symbol': '<UNK>',\n",
      "                                         'vocab_file': None},\n",
      "                         'set': {   'fill_value': '<UNK>',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000,\n",
      "                                    'tokenizer': 'space'},\n",
      "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
      "                         'stratify': None,\n",
      "                         'text': {   'char_most_common': 70,\n",
      "                                     'char_sequence_length_limit': 1024,\n",
      "                                     'char_tokenizer': 'characters',\n",
      "                                     'char_vocab_file': None,\n",
      "                                     'fill_value': '<UNK>',\n",
      "                                     'lowercase': True,\n",
      "                                     'missing_value_strategy': 'fill_with_const',\n",
      "                                     'padding': 'right',\n",
      "                                     'padding_symbol': '<PAD>',\n",
      "                                     'unknown_symbol': '<UNK>',\n",
      "                                     'word_most_common': 20000,\n",
      "                                     'word_sequence_length_limit': 256,\n",
      "                                     'word_tokenizer': 'space_punct',\n",
      "                                     'word_vocab_file': None},\n",
      "                         'timeseries': {   'fill_value': '',\n",
      "                                           'missing_value_strategy': 'fill_with_const',\n",
      "                                           'padding': 'right',\n",
      "                                           'padding_value': 0,\n",
      "                                           'timeseries_length_limit': 256,\n",
      "                                           'tokenizer': 'space'},\n",
      "                         'vector': {   'fill_value': '',\n",
      "                                       'missing_value_strategy': 'fill_with_const'}},\n",
      "    'training': {   'batch_size': 128,\n",
      "                    'bucketing_field': None,\n",
      "                    'decay': False,\n",
      "                    'decay_rate': 0.96,\n",
      "                    'decay_steps': 10000,\n",
      "                    'dropout_rate': 0.0,\n",
      "                    'early_stop': 5,\n",
      "                    'epochs': 100,\n",
      "                    'eval_batch_size': 0,\n",
      "                    'gradient_clipping': None,\n",
      "                    'increase_batch_size_on_plateau': 0,\n",
      "                    'increase_batch_size_on_plateau_max': 512,\n",
      "                    'increase_batch_size_on_plateau_patience': 5,\n",
      "                    'increase_batch_size_on_plateau_rate': 2,\n",
      "                    'learning_rate': 0.001,\n",
      "                    'learning_rate_warmup_epochs': 1,\n",
      "                    'optimizer': {   'beta1': 0.9,\n",
      "                                     'beta2': 0.999,\n",
      "                                     'epsilon': 1e-08,\n",
      "                                     'type': 'adam'},\n",
      "                    'reduce_learning_rate_on_plateau': 0,\n",
      "                    'reduce_learning_rate_on_plateau_patience': 5,\n",
      "                    'reduce_learning_rate_on_plateau_rate': 0.5,\n",
      "                    'regularization_lambda': 0,\n",
      "                    'regularizer': 'l2',\n",
      "                    'staircase': False,\n",
      "                    'validation_field': 'combined',\n",
      "                    'validation_measure': 'loss'}}\n",
      "\n",
      "\n",
      "Found hdf5 and json with the same filename of the csv, using them instead\n",
      "Using full hdf5 and json\n",
      "Loading data from: training_set.hdf5\n",
      "Loading metadata from: training_set.json\n",
      "Training set: 3333\n",
      "Validation set: 443\n",
      "Test set: 937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "2020-10-14 19:42:45.202176: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-10-14 19:42:45.223233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3801000000 Hz\n",
      "2020-10-14 19:42:45.223453: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x531c7b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-14 19:42:45.223484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\n",
      "Epoch   1\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.34it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.18it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  7.00it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.61it/s]\n",
      "Took 26.4021s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0753 │     0.9853 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.1131 │     0.9684 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.1008 │     0.9733 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   2\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:19<00:00,  1.36it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.11it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  7.06it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.58it/s]\n",
      "Took 26.1346s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0312 │     0.9919 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0704 │     0.9797 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0642 │     0.9819 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   3\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:19<00:00,  1.36it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.25it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  7.06it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.62it/s]\n",
      "Took 25.9099s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0155 │     0.9925 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0882 │     0.9639 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0826 │     0.9680 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   4\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.35it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.15it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.99it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.53it/s]\n",
      "Took 26.2374s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0010 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0289 │     0.9887 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0343 │     0.9872 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   5\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.32it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.00it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.70it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.42it/s]\n",
      "Took 26.8204s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0008 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0262 │     0.9910 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0313 │     0.9893 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   6\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.31it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  5.78it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.86it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.39it/s]\n",
      "Took 27.4743s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0011 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0271 │     0.9910 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0318 │     0.9904 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   7\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.31it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.19it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.92it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.61it/s]\n",
      "Took 26.7018s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0004 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0265 │     0.9910 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0322 │     0.9872 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 2 epochs ago\n",
      "\n",
      "\n",
      "Epoch   8\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.32it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.23it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.96it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.47it/s]\n",
      "Took 26.6354s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0004 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0253 │     0.9910 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0312 │     0.9904 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   9\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:19<00:00,  1.35it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.05it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  7.01it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.67it/s]\n",
      "Took 26.1829s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0002 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0317 │     0.9865 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0372 │     0.9840 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  10\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.29it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  5.89it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  7.08it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.63it/s]\n",
      "Took 27.3242s\n",
      "╒══════════╤════════╤════════════╤═════════════╕\n",
      "│ label    │   loss │   accuracy │   hits_at_k │\n",
      "╞══════════╪════════╪════════════╪═════════════╡\n",
      "│ training │ 0.0002 │     1.0000 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ vali     │ 0.0307 │     0.9887 │      1.0000 │\n",
      "├──────────┼────────┼────────────┼─────────────┤\n",
      "│ test     │ 0.0364 │     0.9851 │      1.0000 │\n",
      "╘══════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 2 epochs ago\n",
      "\n",
      "\n",
      "Epoch  11\n",
      "Training:  52%|█████████████████                | 14/27 [00:11<00:10,  1.29it/s]"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!rm -Rf results # Removes past runs\n",
    "!ludwig experiment \\\n",
    "--data_csv training_set.csv \\\n",
    "--model_definition_file model_definition.yaml \\\n",
    "--output_directory ./results\n",
    "!date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visualize\n",
    "# The following would create pdfs of the visualization\n",
    "# !ludwig visualize -v learning_curves -trs results/experiment_run_0/training_statistics.json -tes results/experiment_run_0/test_statistics.json -od .\n",
    "\n",
    "from ludwig import visualize\n",
    "import json\n",
    "\n",
    "with open('./results/experiment_run/training_statistics.json') as f:\n",
    "    data = json.load(f)\n",
    " \n",
    "visualize.learning_curves(data, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
