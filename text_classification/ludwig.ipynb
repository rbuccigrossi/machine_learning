{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ludwig test\n",
    "\n",
    "\n",
    "``` bash\n",
    "# Get ludwig\n",
    "#Library prereq for ludwig 2\n",
    "sudo apt-get install libgmp3-dev\n",
    "sudo apt-get install libsndfile1\n",
    "# Now install ludwig (version 0.2 works with tensorflow 1.14 on python \n",
    "pip3 install ludwig==0.2\n",
    "```\n",
    "\n",
    "## Splitting Moby Dick and Romeo and Juliet\n",
    "\n",
    "``` bash\n",
    "# Word wrap at 80 characters\n",
    "fold -w 80 -s moby_dick.txt\n",
    "\n",
    "# Now split the text every 5 lines, and prefix with a label\n",
    "cd book_data\n",
    "split -d -l 5 ../romeo_juliet.txt RJ_\n",
    "split -d -l 5 ../moby_dick.txt MD_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our Labeled Text into a CSV (via DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Cook,” said Stubb, rapidly lifting a rather r...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nA short space elapsed, and up into this nois...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said humorous Stubb one day, “he can never be ...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and portion off their nieces with a few porpoi...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as ordinary fish possess what is called a swim...</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JULIET\\nNow, good sweet nurse,--O Lord, why lo...</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  “Cook,” said Stubb, rapidly lifting a rather r...    MD\n",
       "1  \\nA short space elapsed, and up into this nois...    MD\n",
       "2  said humorous Stubb one day, “he can never be ...    MD\n",
       "3  and portion off their nieces with a few porpoi...    MD\n",
       "4  as ordinary fish possess what is called a swim...    MD\n",
       "5  JULIET\\nNow, good sweet nurse,--O Lord, why lo...    RJ"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import the full collection of labeled texts\n",
    "#\n",
    "# All of the text is in \"book_data\" and begins with a label \"MD_\" for \"Moby Dick\" \n",
    "# or \"RJ_\" for Romeo and Juliet\n",
    "#\n",
    "\n",
    "filenames = []\n",
    "texts = []\n",
    "labels = []\n",
    "path = \"book_data\"\n",
    "# First let's\n",
    "for file in os.listdir(path):\n",
    "    if not file.startswith('.'):\n",
    "        filenames.append(file)\n",
    "        labels.append(file.split('_')[0])\n",
    "        with open(os.path.join(path,file), 'r') as f:\n",
    "            texts.append(f.read())\n",
    "\n",
    "# We read the text and insert it into a pandas dataframe. Sklearn uses dataframes\n",
    "# because it is a table class with nice analysis methods\n",
    "labeled_data = pd.DataFrame()\n",
    "labeled_data['text'] = texts\n",
    "labeled_data['label'] = labels\n",
    "\n",
    "# Display the first 6 rows to show that we successfully read it\n",
    "labeled_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,label\r\n",
      "\"“Cook,” said Stubb, rapidly lifting a rather reddish morsel to his mouth, \r\n",
      "“don’t you think this steak is rather overdone? You’ve been beating this \r\n",
      "steak too much, cook; it’s too tender. Don’t I always say that to be good, \r\n",
      "a whale-steak must be tough? There are those sharks now over the side, don’t \r\n",
      "you see they prefer it tough and rare? What a shindy they are kicking up! Cook, \r\n",
      "\",MD\r\n",
      "\"\r\n",
      "A short space elapsed, and up into this noiselessness came Ahab alone from his \r\n",
      "cabin. Taking a few turns on the quarter-deck, he paused to gaze over the side, \r\n"
     ]
    }
   ],
   "source": [
    "# Now save the data as a csv file\n",
    "labeled_data.to_csv(\"training_set.csv\", index=False)\n",
    "! head training_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ludwig to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 12 15:00:56 EDT 2020\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/butch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/initializer_modules.py:34: The name tf.initializers.he_normal is deprecated. Please use tf.compat.v1.initializers.he_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/initializer_modules.py:35: The name tf.initializers.he_uniform is deprecated. Please use tf.compat.v1.initializers.he_uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/initializer_modules.py:36: The name tf.initializers.lecun_normal is deprecated. Please use tf.compat.v1.initializers.lecun_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/initializer_modules.py:37: The name tf.initializers.lecun_uniform is deprecated. Please use tf.compat.v1.initializers.lecun_uniform instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/recurrent_modules.py:60: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
      "\n",
      "███████████████████████\n",
      "█ █ █ █  ▜█ █ █ █ █   █\n",
      "█ █ █ █ █ █ █ █ █ █ ███\n",
      "█ █   █ █ █ █ █ █ █ ▌ █\n",
      "█ █████ █ █ █ █ █ █ █ █\n",
      "█     █  ▟█     █ █   █\n",
      "███████████████████████\n",
      "ludwig v0.2 - Experiment\n",
      "\n",
      "Experiment name: experiment\n",
      "Model name: run\n",
      "Output path: ./results/experiment_run_1\n",
      "\n",
      "\n",
      "ludwig_version: '0.2'\n",
      "command: ('/home/butch/.local/bin/ludwig experiment --data_csv training_set.csv '\n",
      " '--model_definition_file model_definition.yaml --output_directory ./results')\n",
      "commit_hash: '7506b03371b1'\n",
      "random_seed: 42\n",
      "input_data: 'training_set.csv'\n",
      "model_definition: {   'combiner': {'type': 'concat'},\n",
      "    'input_features': [   {   'encoder': 'parallel_cnn',\n",
      "                              'level': 'word',\n",
      "                              'name': 'text',\n",
      "                              'tied_weights': None,\n",
      "                              'type': 'text'}],\n",
      "    'output_features': [   {   'dependencies': [],\n",
      "                               'loss': {   'class_similarities_temperature': 0,\n",
      "                                           'class_weights': 1,\n",
      "                                           'confidence_penalty': 0,\n",
      "                                           'distortion': 1,\n",
      "                                           'labels_smoothing': 0,\n",
      "                                           'negative_samples': 0,\n",
      "                                           'robust_lambda': 0,\n",
      "                                           'sampler': None,\n",
      "                                           'type': 'softmax_cross_entropy',\n",
      "                                           'unique': False,\n",
      "                                           'weight': 1},\n",
      "                               'name': 'label',\n",
      "                               'reduce_dependencies': 'sum',\n",
      "                               'reduce_input': 'sum',\n",
      "                               'top_k': 3,\n",
      "                               'type': 'category'}],\n",
      "    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\n",
      "                                      'audio_file_length_limit_in_s': 7.5,\n",
      "                                      'in_memory': True,\n",
      "                                      'missing_value_strategy': 'backfill',\n",
      "                                      'norm': None,\n",
      "                                      'padding_value': 0},\n",
      "                         'bag': {   'fill_value': '',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000,\n",
      "                                    'tokenizer': 'space'},\n",
      "                         'binary': {   'fill_value': 0,\n",
      "                                       'missing_value_strategy': 'fill_with_const'},\n",
      "                         'category': {   'fill_value': '<UNK>',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 10000},\n",
      "                         'date': {   'datetime_format': None,\n",
      "                                     'fill_value': '',\n",
      "                                     'missing_value_strategy': 'fill_with_const'},\n",
      "                         'force_split': False,\n",
      "                         'h3': {   'fill_value': 576495936675512319,\n",
      "                                   'missing_value_strategy': 'fill_with_const'},\n",
      "                         'image': {   'in_memory': True,\n",
      "                                      'missing_value_strategy': 'backfill',\n",
      "                                      'resize_method': 'interpolate',\n",
      "                                      'scaling': 'pixel_normalization'},\n",
      "                         'numerical': {   'fill_value': 0,\n",
      "                                          'missing_value_strategy': 'fill_with_const',\n",
      "                                          'normalization': None},\n",
      "                         'sequence': {   'fill_value': '',\n",
      "                                         'lowercase': False,\n",
      "                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                         'most_common': 20000,\n",
      "                                         'padding': 'right',\n",
      "                                         'padding_symbol': '<PAD>',\n",
      "                                         'sequence_length_limit': 256,\n",
      "                                         'tokenizer': 'space',\n",
      "                                         'unknown_symbol': '<UNK>',\n",
      "                                         'vocab_file': None},\n",
      "                         'set': {   'fill_value': '',\n",
      "                                    'lowercase': False,\n",
      "                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                    'most_common': 10000,\n",
      "                                    'tokenizer': 'space'},\n",
      "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
      "                         'stratify': None,\n",
      "                         'text': {   'char_most_common': 70,\n",
      "                                     'char_sequence_length_limit': 1024,\n",
      "                                     'char_tokenizer': 'characters',\n",
      "                                     'char_vocab_file': None,\n",
      "                                     'fill_value': '',\n",
      "                                     'lowercase': True,\n",
      "                                     'missing_value_strategy': 'fill_with_const',\n",
      "                                     'padding': 'right',\n",
      "                                     'padding_symbol': '<PAD>',\n",
      "                                     'unknown_symbol': '<UNK>',\n",
      "                                     'word_most_common': 20000,\n",
      "                                     'word_sequence_length_limit': 256,\n",
      "                                     'word_tokenizer': 'space_punct',\n",
      "                                     'word_vocab_file': None},\n",
      "                         'timeseries': {   'fill_value': '',\n",
      "                                           'missing_value_strategy': 'fill_with_const',\n",
      "                                           'padding': 'right',\n",
      "                                           'padding_value': 0,\n",
      "                                           'timeseries_length_limit': 256,\n",
      "                                           'tokenizer': 'space'}},\n",
      "    'training': {   'batch_size': 128,\n",
      "                    'bucketing_field': None,\n",
      "                    'decay': False,\n",
      "                    'decay_rate': 0.96,\n",
      "                    'decay_steps': 10000,\n",
      "                    'dropout_rate': 0.0,\n",
      "                    'early_stop': 5,\n",
      "                    'epochs': 100,\n",
      "                    'eval_batch_size': 0,\n",
      "                    'gradient_clipping': None,\n",
      "                    'increase_batch_size_on_plateau': 0,\n",
      "                    'increase_batch_size_on_plateau_max': 512,\n",
      "                    'increase_batch_size_on_plateau_patience': 5,\n",
      "                    'increase_batch_size_on_plateau_rate': 2,\n",
      "                    'learning_rate': 0.001,\n",
      "                    'learning_rate_warmup_epochs': 1,\n",
      "                    'optimizer': {   'beta1': 0.9,\n",
      "                                     'beta2': 0.999,\n",
      "                                     'epsilon': 1e-08,\n",
      "                                     'type': 'adam'},\n",
      "                    'reduce_learning_rate_on_plateau': 0,\n",
      "                    'reduce_learning_rate_on_plateau_patience': 5,\n",
      "                    'reduce_learning_rate_on_plateau_rate': 0.5,\n",
      "                    'regularization_lambda': 0,\n",
      "                    'regularizer': 'l2',\n",
      "                    'staircase': False,\n",
      "                    'validation_field': 'combined',\n",
      "                    'validation_measure': 'loss'}}\n",
      "\n",
      "\n",
      "Found hdf5 and json with the same filename of the csv, using them instead\n",
      "Using full hdf5 and json\n",
      "Loading data from: training_set.hdf5\n",
      "/home/butch/.local/lib/python3.6/site-packages/ludwig/data/preprocessing.py:246: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  dataset[text_data_field] = hdf5_data[text_data_field].value\n",
      "/home/butch/.local/lib/python3.6/site-packages/ludwig/data/preprocessing.py:258: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  output_feature['name']].value\n",
      "/home/butch/.local/lib/python3.6/site-packages/ludwig/data/preprocessing.py:269: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  split = hdf5_data['split'].value\n",
      "Loading metadata from: training_set.json\n",
      "Training set: 3333\n",
      "Validation set: 443\n",
      "Test set: 937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:137: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:141: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:144: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/inputs.py:60: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/inputs.py:60: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/features/category_feature.py:451: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/loss_modules.py:291: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/modules/optimization_modules.py:84: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:211: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:216: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "2020-10-12 15:00:59.394500: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-10-12 15:00:59.419320: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3801000000 Hz\n",
      "2020-10-12 15:00:59.419545: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x692b580 executing computations on platform Host. Devices:\n",
      "2020-10-12 15:00:59.419567: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-10-12 15:00:59.628038: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "WARNING:tensorflow:From /home/butch/.local/lib/python3.6/site-packages/ludwig/models/model.py:422: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "\n",
      "Epoch   1\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:21<00:00,  1.26it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.12it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.89it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.58it/s]\n",
      "Took 27.6556s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.1201 │     0.9505 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.2269 │     0.9187 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.1981 │     0.9210 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   2\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:22<00:00,  1.19it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  5.86it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.74it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.47it/s]\n",
      "Took 29.4118s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0202 │     0.9952 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0658 │     0.9774 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0661 │     0.9808 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   3\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:22<00:00,  1.21it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.07it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.81it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.27it/s]\n",
      "Took 28.6140s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0005 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0399 │     0.9887 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0320 │     0.9872 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   4\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.32it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.22it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.89it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.49it/s]\n",
      "Took 26.6092s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0007 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0287 │     0.9910 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0252 │     0.9925 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Validation loss on combined improved, model saved\n",
      "\n",
      "\n",
      "Epoch   5\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.32it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.19it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.95it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.53it/s]\n",
      "Took 26.6041s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0002 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0360 │     0.9887 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0288 │     0.9893 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 1 epoch ago\n",
      "\n",
      "\n",
      "Epoch   6\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.32it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.17it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.85it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.37it/s]\n",
      "Took 26.6290s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0003 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0294 │     0.9910 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0248 │     0.9893 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 2 epochs ago\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch   7\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.31it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.16it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.92it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.54it/s]\n",
      "Took 26.8506s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0001 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0381 │     0.9887 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0302 │     0.9893 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 3 epochs ago\n",
      "\n",
      "\n",
      "Epoch   8\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.33it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.20it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.96it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.49it/s]\n",
      "Took 26.5185s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0001 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0366 │     0.9887 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0288 │     0.9893 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 4 epochs ago\n",
      "\n",
      "\n",
      "Epoch   9\n",
      "Training: 100%|█████████████████████████████████| 27/27 [00:20<00:00,  1.31it/s]\n",
      "Evaluation train: 100%|█████████████████████████| 27/27 [00:04<00:00,  6.00it/s]\n",
      "Evaluation vali : 100%|███████████████████████████| 4/4 [00:00<00:00,  6.49it/s]\n",
      "Evaluation test : 100%|███████████████████████████| 8/8 [00:01<00:00,  6.32it/s]\n",
      "Took 27.7172s\n",
      "╒═════════╤════════╤════════════╤═════════════╕\n",
      "│ label   │   loss │   accuracy │   hits_at_k │\n",
      "╞═════════╪════════╪════════════╪═════════════╡\n",
      "│ train   │ 0.0001 │     1.0000 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ vali    │ 0.0397 │     0.9887 │      1.0000 │\n",
      "├─────────┼────────┼────────────┼─────────────┤\n",
      "│ test    │ 0.0311 │     0.9883 │      1.0000 │\n",
      "╘═════════╧════════╧════════════╧═════════════╛\n",
      "Last improvement of loss on combined happened 5 epochs ago\n",
      "\n",
      "EARLY STOPPING due to lack of validation improvement, it has been 5 epochs since last validation accuracy improvement\n",
      "\n",
      "Best validation model epoch: 4\n",
      "Best validation model loss on validation set combined: 0.028748816615033635\n",
      "Best validation model loss on test set combined: 0.02520095907636742\n",
      "\n",
      "Finished: experiment_run\n",
      "Saved to: ./results/experiment_run_1\n",
      "\n",
      "╒═════════╕\n",
      "│ PREDICT │\n",
      "╘═════════╛\n",
      "\n",
      "Evaluation: 100%|█████████████████████████████████| 8/8 [00:01<00:00,  6.09it/s]\n",
      "\n",
      "===== label =====\n",
      "accuracy: 0.9882604055496265\n",
      "hits_at_k: 1.0\n",
      "loss: 0.031057823365882215\n",
      "overall_stats: { 'avg_f1_score_macro': 0.9803725913244835,\n",
      "  'avg_f1_score_micro': 0.9882604055496265,\n",
      "  'avg_f1_score_weighted': 0.9881674515099362,\n",
      "  'avg_precision_macro': 0.9881958635209611,\n",
      "  'avg_precision_micro': 0.9882604055496265,\n",
      "  'avg_precision_weighted': 0.9882604055496265,\n",
      "  'avg_recall_macro': 0.972973378327709,\n",
      "  'avg_recall_micro': 0.9882604055496265,\n",
      "  'avg_recall_weighted': 0.9882604055496265,\n",
      "  'kappa_score': 0.9607488451622486,\n",
      "  'token_accuracy': 0.9882604055496265}\n",
      "per_class_stats: {<UNK>: {   'accuracy': 1.0,\n",
      "    'f1_score': 0,\n",
      "    'fall_out': 0.0,\n",
      "    'false_discovery_rate': 1.0,\n",
      "    'false_negative_rate': 1.0,\n",
      "    'false_negatives': 0,\n",
      "    'false_omission_rate': 0.0,\n",
      "    'false_positive_rate': 0.0,\n",
      "    'false_positives': 0,\n",
      "    'hit_rate': 0,\n",
      "    'informedness': 0.0,\n",
      "    'markedness': 0.0,\n",
      "    'matthews_correlation_coefficient': 0,\n",
      "    'miss_rate': 1.0,\n",
      "    'negative_predictive_value': 1.0,\n",
      "    'positive_predictive_value': 0,\n",
      "    'precision': 0,\n",
      "    'recall': 0,\n",
      "    'sensitivity': 0,\n",
      "    'specificity': 1.0,\n",
      "    'true_negative_rate': 1.0,\n",
      "    'true_negatives': 937,\n",
      "    'true_positive_rate': 0,\n",
      "    'true_positives': 0},\n",
      "  MD: {   'accuracy': 0.9882604055496265,\n",
      "    'f1_score': 0.992815153494448,\n",
      "    'fall_out': 0.011904761904761862,\n",
      "    'false_discovery_rate': 0.002624671916010457,\n",
      "    'false_negative_rate': 0.011703511053316018,\n",
      "    'false_negatives': 9,\n",
      "    'false_omission_rate': 0.05142857142857138,\n",
      "    'false_positive_rate': 0.011904761904761862,\n",
      "    'false_positives': 2,\n",
      "    'hit_rate': 0.988296488946684,\n",
      "    'informedness': 0.9763917270419222,\n",
      "    'markedness': 0.945946756655418,\n",
      "    'matthews_correlation_coefficient': 0.9610486914930421,\n",
      "    'miss_rate': 0.011703511053316018,\n",
      "    'negative_predictive_value': 0.9485714285714286,\n",
      "    'positive_predictive_value': 0.9973753280839895,\n",
      "    'precision': 0.9973753280839895,\n",
      "    'recall': 0.988296488946684,\n",
      "    'sensitivity': 0.988296488946684,\n",
      "    'specificity': 0.9880952380952381,\n",
      "    'true_negative_rate': 0.9880952380952381,\n",
      "    'true_negatives': 166,\n",
      "    'true_positive_rate': 0.988296488946684,\n",
      "    'true_positives': 760},\n",
      "  RJ: {   'accuracy': 0.9882604055496265,\n",
      "    'f1_score': 0.967930029154519,\n",
      "    'fall_out': 0.011703511053316018,\n",
      "    'false_discovery_rate': 0.05142857142857138,\n",
      "    'false_negative_rate': 0.011904761904761862,\n",
      "    'false_negatives': 2,\n",
      "    'false_omission_rate': 0.002624671916010457,\n",
      "    'false_positive_rate': 0.011703511053316018,\n",
      "    'false_positives': 9,\n",
      "    'hit_rate': 0.9880952380952381,\n",
      "    'informedness': 0.9763917270419222,\n",
      "    'markedness': 0.945946756655418,\n",
      "    'matthews_correlation_coefficient': 0.9610486914930421,\n",
      "    'miss_rate': 0.011904761904761862,\n",
      "    'negative_predictive_value': 0.9973753280839895,\n",
      "    'positive_predictive_value': 0.9485714285714286,\n",
      "    'precision': 0.9485714285714286,\n",
      "    'recall': 0.9880952380952381,\n",
      "    'sensitivity': 0.9880952380952381,\n",
      "    'specificity': 0.988296488946684,\n",
      "    'true_negative_rate': 0.988296488946684,\n",
      "    'true_negatives': 760,\n",
      "    'true_positive_rate': 0.9880952380952381,\n",
      "    'true_positives': 166}}\n",
      "\n",
      "Finished: experiment_run\n",
      "Saved to: ./results/experiment_run_1\n",
      "Mon Oct 12 15:05:11 EDT 2020\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!ludwig experiment \\\n",
    "--data_csv training_set.csv \\\n",
    "--model_definition_file model_definition.yaml \\\n",
    "--output_directory ./results\n",
    "!date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visualize\n",
    "!ludwig visualize -v learning_curves -trs results/experiment_run_0/training_statistics.json -tes results/experiment_run_0/test_statistics.json -od .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_curves_combined_accuracy.pdf  learning_curves_label_hits_at_k.pdf\r\n",
      "learning_curves_combined_loss.pdf      learning_curves_label_loss.pdf\r\n",
      "learning_curves_label_accuracy.pdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"learning_curves_combined_accuracy.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc3a1fc0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"learning_curves_combined_accuracy.pdf\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"learning_curves_combined_loss.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc3a1fc390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"learning_curves_combined_loss.pdf\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
