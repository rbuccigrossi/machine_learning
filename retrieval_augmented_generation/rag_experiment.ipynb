{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment Configuration\n",
    "### Install Python 3.10 on Ubuntu\n",
    "Follow https://computingforgeeks.com/how-to-install-python-on-ubuntu-linux-system/ .\n",
    "\n",
    "    sudo apt-get update\n",
    "    sudo apt install software-properties-common -y\n",
    "    sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "    sudo apt install python3.10\n",
    "\n",
    "### Create Python 3.10 virtualenv in ~/py310\n",
    "\n",
    "    cd ~\n",
    "    pip3 install virtualenv\n",
    "    virtualenv --python=/usr/bin/python3.10 py310\n",
    "    source py310/bin/activate\n",
    "    pip list # Show packages\n",
    "    pip install --upgrade pip\n",
    "\n",
    "### Add py310 virtualenv to Jupyter\n",
    "\n",
    "    ipython kernel install --user --name py310\n",
    "\n",
    "Now select the kernel when running this notebook.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 (main, Apr  5 2023, 14:15:30) [GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation with Pinecone & ChatGPT\n",
    "More information at https://www.pinecone.io/learn/openai-gen-qa/\n",
    "Github repository: https://github.com/pinecone-io/examples/tree/master/generation/generative-qa/openai/gen-qa-openai\n",
    "\n",
    "## Install dependencies\n",
    "   \n",
    "    pip install -qU openai pinecone-client datasets tqdm\n",
    "\n",
    "## Set OPENAI_API_KEY before running jupyter\n",
    "\n",
    "You need an API key set up from: https://platform.openai.com/account/api-keys\n",
    "\n",
    "    export OPENAI_API_KEY=\"secret key from site\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
    "\n",
    "# openai.Engine.list()  # check we have authenticated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query with ChatGPT 3.5\n",
    "Now run a query with ChatGPT 3.5-turbo. Using ChatCompletion.create ( https://platform.openai.com/docs/api-reference/chat/create ) you can construct a chat history (with memory) for the chatbot. (The limit is 4096 tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main components of the zero trust architecture are:\n",
      "\n",
      "1. Identity and access management (IAM): This component verifies the identity of users, devices, and applications before granting access to resources.\n",
      "\n",
      "2. Multi-factor authentication (MFA): MFA adds an extra layer of security by requiring users to provide more than one form of authentication before granting access.\n",
      "\n",
      "3. Network segmentation: This component separates the network into small, isolated segments to minimize the attack surface.\n",
      "\n",
      "4. Micro-segmentation: Micro-segmentation limits applications and services to specific users and devices, reducing the risk of a data breach.\n",
      "\n",
      "5. Least privileged access: This component provides users with access to only the resources they need to perform their job functions, reducing the risk of accidental or intentional data breaches.\n",
      "\n",
      "6. Continuous monitoring and analytics: This component monitors all activity within the network, including user behavior and system access, to detect and prevent security breaches.\n",
      "\n",
      "7. Policy-based access control: This component enforces policies that control access to resources based on user identity, device type, and location.\n",
      "\n",
      "These components work together to create a security architecture that assumes all traffic is untrusted and authenticates every access request. The zero trust architecture greatly enhances security by reducing the attack surface, improving visibility, and minimizing the risk of data breaches.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of the zero trust architecture, and what do they do?\"\n",
    "\n",
    "# now query GPT 3.5 WITHOUT context\n",
    "res = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\" : query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting an answer by providing more context\n",
    "\n",
    "The answer is very good in terms of Zero Trust, but not accurate for the NIST model specifically (the NIST ZTA is defined in NIST SP 800-207 https://csrc.nist.gov/publications/detail/sp/800-207/final ).\n",
    "\n",
    "Can we create context (provide additional information) in the chat that will allow the chatbot to answer the question correctly for the NIST ZTA? The general approach to this is **\"retrieval-augmented generation\"** in which we use a vector database to store documents and pull relevant text into a context that we provide the chatbot.\n",
    "\n",
    "### Connect and Authenticate to Pinecone\n",
    "Set OPENAI_API_KEY before running jupyter\n",
    "You need an API key set up from: https://app.pinecone.io\n",
    "     export PINECONE_API_KEY=\"API key from pinecone.io\"\n",
    "     export PINECONE_ENVIRONMENT=\"the environment code (next to the API key) from pinecone.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
    "# find your environment next to the api key in pinecone console\n",
    "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
    "\n",
    "pinecone.init(api_key=api_key, enviroment=env)\n",
    "# pinecone.whoami()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sample embedding so that we know the embedding length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\"This is sample test that will determine the length\"],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "embedding_length = len(res['data'][0]['embedding'])\n",
    "embedding_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'openai-nist-sp'\n",
    "\n",
    "# Create the index if it doesn't exist already\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=embedding_length,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['document']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 13}},\n",
       " 'total_vector_count': 13}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats \n",
    "index_stats = index.describe_index_stats()\n",
    "index_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read NIST SP 800-207 and add to the vector database if it is empty\n",
    "\n",
    "entry_word_max = 1500\n",
    "entries=[] # Collect our entries\n",
    "\n",
    "if (index_stats.total_vector_count == 0):\n",
    "    with open('NIST.SP.800-207.txt', 'r') as file:\n",
    "        book = file.read()\n",
    "\n",
    "    # Collect the text by paragraph into blocks of entry_word_max words\n",
    "    entry = \"\"\n",
    "    entry_word_count = 0\n",
    "    for line in book.split('SECTION'):\n",
    "        line_word_count = len(line.split())\n",
    "        if ((line_word_count + entry_word_count) > entry_word_max):\n",
    "            entries.append(entry)\n",
    "            entry = \"\"\n",
    "            entry_word_count = 0\n",
    "        entry += line + \" \"\n",
    "        entry_word_count += line_word_count\n",
    "\n",
    "    # record last entry if not empty\n",
    "    if (entry_word_count > 0):\n",
    "        entries.append(entry)\n",
    "\n",
    "len(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the embeddings\n",
    "\n",
    "if (len(entries) > 0):\n",
    "    embeddings = openai.Embedding.create(input=entries, engine=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now zip up the embeddings and insert!\n",
    "\n",
    "if (len(entries) > 0):\n",
    "    # Object is of the form (id, vector, meta_data)\n",
    "    to_upsert = [('800-207-'+str(i), embeddings['data'][i]['embedding'],{'document':'NIST SP 800-207', 'text':entries[i]}) \n",
    "                 for i in range(len(entries)) ]\n",
    "    index.upsert(vectors=to_upsert)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a to search our index!\n",
    "query = \"What are the main components of the zero trust architecture, and what do they do?\"\n",
    "qe = openai.Embedding.create(input=[query], engine=embed_model)\n",
    "res = index.query(qe['data'][0]['embedding'], top_k=2, include_metadata=True)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = ( \n",
    "    \"This is initial context for a question below\\n\\n\" +\n",
    "    \"Context: \\n\" +\n",
    "    res['matches'][0]['metadata']['text'] + \"\\n\\n\"\n",
    ")\n",
    "context_2 = ( \n",
    "    \"This is additional context for a question below\\n\\n\" +\n",
    "    \"Context: \\n\" +\n",
    "    res['matches'][1]['metadata']['text'] + \"\\n\\n\"\n",
    ")\n",
    "question = (\n",
    "    \"Answer the question based upon the previous context. If the answer is not clear from the source, \" +\n",
    "    \"state 'I cannot answer based upon the context.'.\\n\\n\" +\n",
    "    \"Question: \" + query\n",
    ")\n",
    "\n",
    "# [context_1, context_2, question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main components of the zero trust architecture are policy engine (PE), policy administrator (PA), and policy enforcement point (PEP). The policy engine is responsible for the ultimate decision to grant access to a resource based on enterprise policy and external sources. The policy administrator establishes and/or shuts down the communication path between a subject and a resource, generates session-specific authentication, and is closely tied to the policy engine. The policy enforcement point enables, monitors, and terminates connections between a subject and an enterprise resource and communicates with the policy administrator to forward requests and updates policy. Additionally, there are several data sources that provide policy rules used by the policy engine, including the continuous diagnostics, industry compliance system, threat intelligence feed(s), network and system activity logs, data access policies, enterprise PKI, ID management system, and SIEM system.\n"
     ]
    }
   ],
   "source": [
    "# now query GPT 3.5 WITH context\n",
    "res = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\" : context_1},\n",
    "        {\"role\": \"user\", \"content\" : context_2},\n",
    "        {\"role\": \"user\", \"content\" : question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we are done, delete the index\n",
    "# pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
